 <!DOCTYPE html>
<html lang="en">
<head>
  <title>DMCB</title>
  <meta name="description" content="Selecting the Motion Ground Truth for Loose-fitting Wearables: Benchmarking Optical MoCap Methods.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>Selecting the Motion Ground Truth for Loose-fitting Wearables: Benchmarking Optical MoCap Methods</h1>
    <h4>ISWC 2023 (oral presentation)</h4>
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-3"></div>
    <div class="col-xs-12 col-md-6">
      <h4>
        <a href="https://www.dfki.de/web/ueber-uns/mitarbeiter/person/lara01"><nobr>Lala Ray</nobr></a> &emsp;
        <a href="https://www.dfki.de/web/ueber-uns/mitarbeiter/person/bozh01"><nobr>Bo Zhou</nobr></a> &emsp;
        <a href="https://www.dfki.de/web/ueber-uns/mitarbeiter/person/susu02"><nobr>Sungho Suh</nobr></a> &emsp;
        <a href="https://www.dfki.de/web/ueber-uns/mitarbeiter/person/palu01"><nobr>Paul Lukowicz</nobr></a>
      </h4>
      <nobr>DFKI Kaiserslautern</nobr> and <nobr> RPTU </nobr>
      <nobr>Kaiserslautern, Germany</nobr>
    </div>
    <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
      <a href="https://arxiv.org/pdf/2307.11881.pdf" style="color:inherit">
        <i class="fa fa-file-pdf-o fa-4x"></i></a> 
    </div>
    <div class="hidden-xs hidden-sm col-md-2" style="text-align:left; margin-left:0px;">
      <a href="https://github.com/lalasray/DMCB" style="color:inherit">
        <i class="fa fa-github fa-4x"></i></a>
    </div>
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="resrc/teaser.png" alt="teaser.png" class="text-center" style="width: 100%; max-width: 1100px">
  <h3 style="text-align:center; padding-top:1rem">
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
    To aid smart wearable researchers in selecting optimal ground truth methods for motion capture (MoCap) across all loose garment types, we introduce a benchmark: DrapeMoCapBench (DMCB). 
    This benchmark is tailored to assess optical marker-based and marker-less MoCap performance. 
    While high-cost marker-based systems are recognized as precise standards, they demand skin-tight markers on bony areas for accuracy, which is problematic with loose garments. 
    Conversely, marker-less MoCap methods driven by computer vision models have evolved, requiring only smartphone cameras and being cost-effective. 
    DMCB employs real-world MoCap datasets, conducting 3D physics simulations with diverse variables: six drape levels, three motion intensities, and six body type-gender combinations. 
    This benchmarks advanced marker-based and marker-less MoCap techniques, identifying the superior approach for distinct scenarios. 
    When evaluating casual loose garments, both methods exhibit notable performance degradation (>10cm). 
    However, for everyday activities involving basic and swift motions, marker-less MoCap slightly surpasses marker-based alternatives. 
    This renders it an advantageous and economical choice for wearable studies.
  </p>

  <h3>Video</h3>
  <hr/>
  <div class="row" style="text-align:center">
    <div class="col-xs-12 text-center">
      <h4>Presentation (5min)</h4>
      <div class="embed-responsive embed-responsive-16by9" style="text-align:center">
        <iframe class="embed-responsive-item text-center" src="https://www.youtube.com/embed/2kn5ndsvf08?si=jezj5xmbe-htDz5s" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);" allowfullscreen></iframe>
      </div>
    </div>
  </div>

  <h3>Results</h3>
  <hr/>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4>Joint Position & Angle Analyis</h4>
      <img src="resrc/prototypes.png" alt="firenze.jpg" class="text-center" style="width: 100%; max-width: 1100px">
  </div>

  <h3>BibTeX</h3>
  <hr/>
    If you find this work useful for your research, please cite:
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
@inproceedings{10.1145/3594738.3611359,
  author = {Ray, Lala Shakti Swarup and Zhou, Bo and Suh, Sungho and Lukowicz, Paul},
  title = {Selecting the Motion Ground Truth for Loose-Fitting Wearables: Benchmarking Optical MoCap Methods},
  year = {2023},
  isbn = {9798400701993},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3594738.3611359},
  doi = {10.1145/3594738.3611359},
  abstract = {To aid smart wearable researchers in selecting optimal ground truth methods for motion capture (MoCap) across all loose garment types, we introduce a benchmark: DrapeMoCapBench (DMCB). This benchmark is tailored to assess optical marker-based and marker-less MoCap performance. While high-cost marker-based systems are recognized as precise standards, they demand skin-tight markers on bony areas for accuracy, which is problematic with loose garments. Conversely, marker-less MoCap methods driven by computer vision models have evolved, requiring only smartphone cameras and being cost-effective. DMCB employs real-world MoCap datasets, conducting 3D physics simulations with diverse variables: six drape levels, three motion intensities, and six body type-gender combinations. This benchmarks advanced marker-based and marker-less MoCap techniques, identifying the superior approach for distinct scenarios. When evaluating casual loose garments, both methods exhibit notable performance degradation (&gt;10cm). However, for everyday activities involving basic and swift motions, marker-less MoCap slightly surpasses marker-based alternatives. This renders it an advantageous and economical choice for wearable studies.},
  booktitle = {Proceedings of the 2023 ACM International Symposium on Wearable Computers},
  pages = {27â€“32},
  numpages = {6},
  location = {Cancun, Quintana Roo, Mexico},
  series = {ISWC '23}
  }</pre>
      </div>
    </div>

  <h3>Further information</h3>
  <hr/>
  If you like this project, please check out other related works from our group:
  <h4>Follow-ups</h4>
  <ul>
    <li>
      <a >OptiMocapBench (ToDo)</a>
    </li>
  </ul>

  <h3>Acknowledgements</h3>
  <hr/>
  <p>
    The research reported in this paper was supported by the BMBF (German Federal Ministry of Education and Research) in the project VidGenSense (01IW21003). It was also funded by Carl-Zeiss Stiftung under the Sustainable Embedded AI project (P2021-02-009).
  </p>
</div>

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  You can find my other works in my
  <a href="https://www.researchgate.net/profile/Lala-Shakti-Swarup-Ray-2">ResearchGate</a> profile.
  </p>
</div>

</body>
</html> 
