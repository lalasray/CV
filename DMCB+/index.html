 <!DOCTYPE html>
<html lang="en">
<head>
  <title>DMCB+</title>
  <meta name="description" content="A Comprehensive Evaluation of Marker-Based, Markerless Methods for Loose Garment Scenarios in Varying Camera Configurations.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>DMCB+: A Comprehensive Evaluation of Marker-Based, Markerless Methods for Loose Garment Scenarios in Varying Camera Configurations</h1>
    <h4>Frontier Computer Science, Mobile and Ubiquitous Computing</h4>
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-3"></div>
    <div class="col-xs-12 col-md-6">
      <h4>
        <a href="https://www.dfki.de/web/ueber-uns/mitarbeiter/person/lara01"><nobr>Lala Ray</nobr></a> &emsp;
        <a href="https://www.dfki.de/web/ueber-uns/mitarbeiter/person/bozh01"><nobr>Bo Zhou</nobr></a> &emsp;
        <a href="https://www.dfki.de/web/ueber-uns/mitarbeiter/person/susu02"><nobr>Sungho Suh</nobr></a> &emsp;
        <a href="https://www.dfki.de/web/ueber-uns/mitarbeiter/person/palu01"><nobr>Paul Lukowicz</nobr></a>
      </h4>
      <nobr>DFKI Kaiserslautern</nobr> and <nobr> RPTU </nobr>
      <nobr>Kaiserslautern, Germany</nobr>
    </div>
    <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
      <a href="https://www.frontiersin.org/articles/10.3389/fcomp.2024.1379925/full" style="color:inherit">
        <i class="fa fa-file-pdf-o fa-4x"></i></a> 
    </div>
    <div class="hidden-xs hidden-sm col-md-2" style="text-align:left; margin-left:0px;">
      <a href="https://github.com/lalasray/DMCB" style="color:inherit">
        <i class="fa fa-github fa-4x"></i></a>
    </div>
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="resrc/teaser.png" alt="teaser.png" class="text-center" style="width: 100%; max-width: 1100px">
  <h3 style="text-align:center; padding-top:1rem">
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
    In support of smart wearable researchers striving to select optimal ground truth methods for motion capture across a spectrum of loose garment types, we present an extended benchmark named DrapeMoCapBench (DMCB+). This augmented benchmark incorporates a more intricate limb-wise Motion Capture (MoCap) accuracy analysis, and enhanced drape calculation, and introduces a novel benchmarking tool that encompasses multicamera deep learning MoCap methods. DMCB+ is specifically designed to evaluate the performance of both optical marker-based and markerless MoCap techniques, taking into account the challenges posed by various loose garment types. While high-cost marker-based systems are acknowledged for their precision, they often require skin-tight markers on bony areas, which can be impractical with loose garments. On the other hand, markerless MoCap methods driven by computer vision models have evolved to be more cost-effective, utilizing smartphone cameras and exhibiting promising results. Utilizing real-world MoCap datasets, DMCB+ conducts 3D physics simulations with a comprehensive set of variables, including six drape levels, three motion intensities, and six body-gender combinations. The extended benchmark provides a nuanced analysis of advanced marker-based and markerless MoCap techniques, highlighting their strengths and weaknesses across distinct scenarios. In particular, DMCB+ reveals that when evaluating casual loose garments, both marker-based and markerless methods exhibit notable performance degradation (>10 cm). However, in scenarios involving everyday activities with basic and swift motions, markerless MoCap outperforms marker-based alternatives. This positions markerless MoCap as an advantageous and economical choice for wearable studies. The inclusion of a multicamera deep learning MoCap method in the benchmarking tool further expands the scope, allowing researchers to assess the capabilities of cutting-edge technologies in diverse motion capture scenarios.
  </p>

  <h3>Results</h3>
  <hr/>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4>Fullbody Joint Position & Angle Analyis</h4>
      <img src="resrc/prototypes_1.png" alt="firenze.jpg" class="text-center" style="width: 70%; max-width: 1100px">
    <h4>Torso Joint Position & Angle Analyis</h4>
      <img src="resrc/prototypes_2.png" alt="firenze.jpg" class="text-center" style="width: 70%; max-width: 1100px">
    <h4>Upper Limb Joint Position & Angle Analyis</h4>
      <img src="resrc/prototypes_3.png" alt="firenze.jpg" class="text-center" style="width: 70%; max-width: 1100px">
    <h4>Lower Limb Joint Position & Angle Analyis</h4>
      <img src="resrc/prototypes_4.png" alt="firenze.jpg" class="text-center" style="width: 70%; max-width: 1100px">
  </div>

  <h3>BibTeX</h3>
  <hr/>
    If you find this work useful for your research, please cite:
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
@ARTICLE{10.3389/fcomp.2024.1379925,

  AUTHOR={Ray, Lala Shakti Swarup and Zhou, Bo and Suh, Sungho and Lukowicz, Paul},   
  TITLE={A comprehensive evaluation of marker-based, markerless methods for loose garment scenarios in varying camera configurations},      
  JOURNAL={Frontiers in Computer Science},     
  VOLUME={6},           
  YEAR={2024},      
  URL={https://www.frontiersin.org/articles/10.3389/fcomp.2024.1379925},      
  DOI={10.3389/fcomp.2024.1379925},      
  ISSN={2624-9898},   
}</pre>
      </div>
    </div>

  <h3>Further information</h3>
  <hr/>
  If you like this project, please check out other related works from our group:
  <h4>Follow-ups</h4>
  <ul>
    <li>
      <a >ClothedPoseEstimation (ToDo)</a>
    </li>
  </ul>

  <h3>Acknowledgements</h3>
  <hr/>
  <p>
    The research reported in this paper was supported by the BMBF (German Federal Ministry of Education and Research) in the project VidGenSense (01IW21003). 
  </p>
</div>

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  You can find my other works in my
  <a href="https://www.researchgate.net/profile/Lala-Shakti-Swarup-Ray-2">ResearchGate</a> profile.
  </p>
</div>

</body>
</html> 
